# **SLM-Book-QA: Small Language Model for Book-Based Question Answering**

## ğŸ“Œ **Project Overview**
This repository contains a **Small Language Model (SLM)-based Question Answering (QA) system** that processes a book as context and accurately answers user queries. The model utilizes **FAISS for efficient text retrieval**, **mpnet embeddings for semantic understanding**, and **Falcon-7B-Instruct** for response generation. This project enables users to ask questions about a given book and receive **concise, accurate, and well-structured answers**.

## ğŸš€ **Features**
âœ… Extracts and preprocesses text from a **PDF book**
âœ… Uses **FAISS** for fast and **relevant text retrieval**
âœ… Embeds text using `all-mpnet-base-v2` for **semantic comprehension**
âœ… Generates **concise and meaningful responses** using `Falcon-7B-Instruct`
âœ… **Interactive Q&A system** with Colab Notebook support
âœ… Easy to run with a **simple and intuitive interface**

## ğŸ›  **Installation**
### **1ï¸âƒ£ Clone the Repository**
```bash
git clone //link of project//
cd SLM-Book-QA
```

### **2ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3ï¸âƒ£ Run colab Notebook**
```bash
colab notebook
```

Open the notebook and run all cells to start the **interactive Q&A system**.

## ğŸ“– **How It Works**
### **1. Text Extraction & Processing**
- The model extracts **raw text** from a given PDF book.
- The text is **cleaned and tokenized** for efficient processing.
- The text is split into **1500-word chunks** to maintain coherence.

### **2. Context Retrieval with FAISS**
- The model uses **FAISS (Facebook AI Similarity Search)** to index text embeddings.
- **SentenceTransformer's `all-mpnet-base-v2`** converts text chunks into **vector embeddings**.
- When a question is asked, the **most relevant text chunks** are retrieved from FAISS.

### **3. Answer Generation with Falcon-7B-Instruct**
- The retrieved text is passed to the **Falcon-7B-Instruct model**.
- A structured **prompt ensures precise and well-formatted answers**.
- The model generates a **concise and direct answer** based on the context.

## ğŸ’¡ **Usage Example**
### **Query:**
```python
answer_question("Define While Loop")
```
### **Expected Output:**
```bash
inside a nested loop ( loop inside another loop), break will terminate the innermost loop.
```

## ğŸ“Œ **Project Structure**
```bash
SLM-Book-QA/
â”‚â”€â”€ data/			 
â”‚â”€â”€ models/			
â”‚â”€â”€ notebook.ipynb		 
â”‚â”€â”€ requirements.txt		 
â”‚â”€â”€ README.md		 
```

## ğŸ“ˆ **Evaluation & Performance**
- **Retrieval Quality:** Ensured by **FAISS indexing & debugging retrieved text**.
- **Answer Accuracy:** Model fine-tuned to generate **short & informative answers**.
- **Efficiency:** Optimized query-response time for smooth user interaction.

## ğŸ† **Key Learnings & Observations**
- **FAISS indexing** significantly improves retrieval speed and relevance.
- **Chunk size (1500 words)** balances **context richness & retrieval efficiency**.
- **Falcon-7B-Instruct** generates **better structured and more natural answers** than Flan-T5.
- **Interactive Jupyter interface** makes it easy to use and test the model.

## ğŸ¤ **Contributing**
If youâ€™d like to improve this project:
1. **Fork the repository**
2. **Create a new branch** 
3. **Commit your changes**
4. **Push to GitHub and create a Pull Request**


ğŸš€ **Ready to take your book-based Q&A to the next level!**

